{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14549993,"sourceType":"datasetVersion","datasetId":9293227},{"sourceId":14550001,"sourceType":"datasetVersion","datasetId":9293233}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"eb1681dd","cell_type":"code","source":"import pandas as pd\nimport re\nfrom sklearn.model_selection import train_test_split\n\n# Load datasets\ntrain_df = pd.read_csv('/kaggle/input/train-csv/train.csv')\ntest_df = pd.read_csv('/kaggle/input/test-csv/test.csv')\n\ndef clean_text_refined(text):\n    if pd.isna(text): return \"\"\n    text = text.lower()\n    # Remove numbers\n    text = re.sub(r'\\d+', '', text)\n    # Keep: letters, spaces, !, and emojis (non-ascii)\n    cleaned = \"\".join([char for char in text if 'a' <= char <= 'z' or char == ' ' or char == '!' or ord(char) > 127])\n    # Normalize whitespace\n    return re.sub(r'\\s+', ' ', cleaned).strip()\n\n# Apply to Train and Test\nfor df in [train_df, test_df]:\n    df['Review Text'] = df['Review Text'].fillna('')\n    df['Review Title'] = df['Review Title'].fillna('')\n    df['combined_text'] = df['Review Title'] + \" \" + df['Review Text']\n    df['cleaned_text'] = df['combined_text'].apply(clean_text_refined)\n\n# 80-20 Split\ntrain_set, val_set = train_test_split(\n    train_df, test_size=0.20, random_state=42, stratify=train_df['Rating']\n)\n\nprint(f\"Preprocessed! Train size: {len(train_set)}, Validation size: {len(val_set)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T15:42:27.749540Z","iopub.execute_input":"2026-01-20T15:42:27.749948Z","iopub.status.idle":"2026-01-20T15:42:27.963643Z","shell.execute_reply.started":"2026-01-20T15:42:27.749908Z","shell.execute_reply":"2026-01-20T15:42:27.962860Z"}},"outputs":[{"name":"stdout","text":"Preprocessed! Train size: 4554, Validation size: 1139\n","output_type":"stream"}],"execution_count":3},{"id":"93fa57ca","cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.utils import class_weight\n\n# Hyperparameters\nMAX_WORDS = 10000  # Only keep the top 10k most frequent words\nMAX_LEN = 100      # Max number of words per review\nEMBEDDING_DIM = 100\n\n# Initialize and fit tokenizer\ntokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(train_set['cleaned_text'])\n\n# Convert text to sequences\nX_train_seq = tokenizer.texts_to_sequences(train_set['cleaned_text'])\nX_val_seq = tokenizer.texts_to_sequences(val_set['cleaned_text'])\n\n# Pad sequences\nX_train_padded = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\nX_val_padded = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n\n# Convert labels to 0-indexed (1-5 -> 0-4) and then to categorical\ny_train_indices = train_set['Rating'].values - 1\ny_val_indices = val_set['Rating'].values - 1\n\ny_train_cat = to_categorical(y_train_indices, num_classes=5)\ny_val_cat = to_categorical(y_val_indices, num_classes=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T15:42:31.729265Z","iopub.execute_input":"2026-01-20T15:42:31.729592Z","iopub.status.idle":"2026-01-20T15:42:31.876352Z","shell.execute_reply.started":"2026-01-20T15:42:31.729564Z","shell.execute_reply":"2026-01-20T15:42:31.875599Z"}},"outputs":[],"execution_count":4},{"id":"bc1969dc","cell_type":"code","source":"# Calculate weights automatically based on frequency\nclasses = np.unique(y_train_indices)\nweights = class_weight.compute_class_weight(class_weight='balanced', \n                                            classes=classes, \n                                            y=y_train_indices)\nclass_weights_dict = dict(zip(classes, weights))\n\nprint(\"Calculated Class Weights:\", class_weights_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T15:42:35.659721Z","iopub.execute_input":"2026-01-20T15:42:35.660050Z","iopub.status.idle":"2026-01-20T15:42:35.669470Z","shell.execute_reply.started":"2026-01-20T15:42:35.660022Z","shell.execute_reply":"2026-01-20T15:42:35.668833Z"}},"outputs":[{"name":"stdout","text":"Calculated Class Weights: {np.int64(0): np.float64(0.6369230769230769), np.int64(1): np.float64(7.404878048780488), np.int64(2): np.float64(5.23448275862069), np.int64(3): np.float64(1.8625766871165643), np.int64(4): np.float64(0.3895637296834902)}\n","output_type":"stream"}],"execution_count":5},{"id":"ae704629","cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, SpatialDropout1D\n\nmodel = Sequential([\n    Embedding(MAX_WORDS, EMBEDDING_DIM, input_length=MAX_LEN),\n    SpatialDropout1D(0.2),\n    Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n    Dense(64, activation='relu'),\n    Dropout(0.3),\n    Dense(5, activation='softmax') # 5 output nodes for ratings 1-5\n])\n\nmodel.compile(loss='categorical_crossentropy', \n              optimizer='adam', \n              metrics=['accuracy'])\n\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T18:16:35.580474Z","iopub.execute_input":"2026-01-19T18:16:35.581099Z","iopub.status.idle":"2026-01-19T18:16:35.619107Z","shell.execute_reply.started":"2026-01-19T18:16:35.581053Z","shell.execute_reply":"2026-01-19T18:16:35.618575Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ spatial_dropout1d_1             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mSpatialDropout1D\u001b[0m)              │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ spatial_dropout1d_1             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SpatialDropout1D</span>)              │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":12},{"id":"3ce30c89","cell_type":"code","source":"EPOCHS = 15\nBATCH_SIZE = 32\n\nhistory = model.fit(\n    X_train_padded, y_train_cat,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    validation_data=(X_val_padded, y_val_cat),\n    class_weight=class_weights_dict, # Apply the class weights here\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T18:16:02.628613Z","iopub.status.idle":"2026-01-19T18:16:02.628832Z","shell.execute_reply.started":"2026-01-19T18:16:02.628725Z","shell.execute_reply":"2026-01-19T18:16:02.628738Z"}},"outputs":[],"execution_count":null},{"id":"b46f7207","cell_type":"code","source":"# Predict probabilities for each class\nval_predictions_probs = model.predict(X_val_padded)\n\n# Get the index of the highest probability (0-4)\nval_predictions_indices = np.argmax(val_predictions_probs, axis=1)\n\n# Convert indices back to original ratings (0-4 -> 1-5)\nval_predictions_ratings = val_predictions_indices + 1\nval_actual_ratings = y_val_indices + 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T18:16:02.629975Z","iopub.status.idle":"2026-01-19T18:16:02.630267Z","shell.execute_reply.started":"2026-01-19T18:16:02.630103Z","shell.execute_reply":"2026-01-19T18:16:02.630117Z"}},"outputs":[],"execution_count":null},{"id":"98b66ed7","cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Calculate Weighted F1-Score\nwf1 = f1_score(val_actual_ratings, val_predictions_ratings, average='weighted')\nprint(f\"Validation Weighted F1-Score: {wf1:.4f}\")\n\n# Detailed Report\nprint(\"\\nClassification Report:\")\nprint(classification_report(val_actual_ratings, val_predictions_ratings))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T18:16:02.630950Z","iopub.status.idle":"2026-01-19T18:16:02.631249Z","shell.execute_reply.started":"2026-01-19T18:16:02.631078Z","shell.execute_reply":"2026-01-19T18:16:02.631101Z"}},"outputs":[],"execution_count":null},{"id":"96851101","cell_type":"code","source":"# Plotting the Confusion Matrix\nplt.figure(figsize=(8, 6))\ncm = confusion_matrix(val_actual_ratings, val_predictions_ratings)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=[1, 2, 3, 4, 5], \n            yticklabels=[1, 2, 3, 4, 5])\nplt.xlabel('Predicted Rating')\nplt.ylabel('Actual Rating')\nplt.title('Validation Confusion Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T18:16:02.632189Z","iopub.status.idle":"2026-01-19T18:16:02.632413Z","shell.execute_reply.started":"2026-01-19T18:16:02.632306Z","shell.execute_reply":"2026-01-19T18:16:02.632319Z"}},"outputs":[],"execution_count":null},{"id":"6a8b292c","cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, SpatialDropout1D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\n# 1. Define Callbacks for Fine-Tuning\n# EarlyStopping stops training when validation loss stops improving to prevent overfitting\nearly_stop = EarlyStopping(\n    monitor='val_loss', \n    patience=3, \n    restore_best_weights=True\n)\n\n# ModelCheckpoint saves the best version of your model during the training process\ncheckpoint = ModelCheckpoint(\n    'best_model.h5', \n    monitor='val_loss', \n    save_best_only=True\n)\n\n# 2. Build the Refined Model Architecture\n# We're adding a second LSTM layer and adjusting dropout for better feature extraction\nmodel = Sequential([\n    Embedding(MAX_WORDS, EMBEDDING_DIM, input_length=MAX_LEN),\n    SpatialDropout1D(0.3),\n    Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),\n    Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3)),\n    Dense(128, activation='relu'),\n    Dropout(0.4),\n    Dense(5, activation='softmax')\n])\n\n# Using a slightly lower learning rate (0.0005) for more stable convergence\noptimizer = Adam(learning_rate=0.0005)\n\nmodel.compile(\n    loss='categorical_crossentropy', \n    optimizer=optimizer, \n    metrics=['accuracy']\n)\n\n# 3. Train the Model with Class Weights and Callbacks\nhistory = model.fit(\n    X_train_padded, \n    y_train_cat,\n    epochs=20, \n    batch_size=64,\n    validation_data=(X_val_padded, y_val_cat),\n    class_weight=class_weights_dict,  # Crucial for the 2, 3, 4 ratings\n    callbacks=[early_stop, checkpoint],\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T18:16:02.633829Z","iopub.status.idle":"2026-01-19T18:16:02.634074Z","shell.execute_reply.started":"2026-01-19T18:16:02.633966Z","shell.execute_reply":"2026-01-19T18:16:02.633980Z"}},"outputs":[],"execution_count":null},{"id":"86f37c71-5050-4e97-8546-b5bae6c0bf7e","cell_type":"code","source":"from sklearn.metrics import classification_report, f1_score, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 1. Load the best weights saved during training\nmodel.load_weights('best_model.h5')\n\n# 2. Predict on validation data\nval_preds_probs = model.predict(X_val_padded)\nval_preds_indices = np.argmax(val_preds_probs, axis=1)\n\n# 3. Calculate Weighted F1-Score\nval_actual = y_val_indices + 1\nval_predicted = val_preds_indices + 1\n\nfinal_f1 = f1_score(val_actual, val_predicted, average='weighted')\nprint(f\"Fine-Tuned Validation Weighted F1-Score: {final_f1_score:.4f}\")\n\n# 4. Detailed Report\nprint(\"\\nFinal Classification Report:\")\nprint(classification_report(val_actual, val_predicted))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T18:16:02.634869Z","iopub.status.idle":"2026-01-19T18:16:02.635157Z","shell.execute_reply.started":"2026-01-19T18:16:02.634999Z","shell.execute_reply":"2026-01-19T18:16:02.635013Z"}},"outputs":[],"execution_count":null},{"id":"d0229e7e-8faa-45d5-adca-229ab106d935","cell_type":"code","source":"# 1. Preprocess and Tokenize the Test Set\n# (Assuming 'test_df' was already cleaned in our earlier preprocessing step)\nX_test_seq = tokenizer.texts_to_sequences(test_df['cleaned_text'])\nX_test_padded = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n\n# 2. Generate Predictions\ntest_preds_probs = model.predict(X_test_padded)\ntest_preds_indices = np.argmax(test_preds_probs, axis=1)\n\n# 3. Convert indices back to 1-5 ratings\ntest_df['Star Rating'] = test_preds_indices + 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T18:16:02.636273Z","iopub.status.idle":"2026-01-19T18:16:02.636561Z","shell.execute_reply.started":"2026-01-19T18:16:02.636441Z","shell.execute_reply":"2026-01-19T18:16:02.636461Z"}},"outputs":[],"execution_count":null},{"id":"c5128975-dc32-43b7-b9aa-287edec64984","cell_type":"code","source":"# Create the submission dataframe with only 'id' and 'Star Rating'\nsubmission = test_df[['id', 'Star Rating']]\n\n# Save to CSV\nsubmission.to_csv('predictions.csv', index=False)\n\nprint(\"Success! 'predictions.csv' has been created.\")\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T18:16:02.637844Z","iopub.status.idle":"2026-01-19T18:16:02.638139Z","shell.execute_reply.started":"2026-01-19T18:16:02.638005Z","shell.execute_reply":"2026-01-19T18:16:02.638021Z"}},"outputs":[],"execution_count":null},{"id":"4e7cf8dc-61d0-4043-81eb-071859e569f4","cell_type":"code","source":"from sklearn.utils import resample\nimport pandas as pd\n\n# Separate classes\ndf_1 = train_set[train_set.Rating == 1]\ndf_2 = train_set[train_set.Rating == 2]\ndf_3 = train_set[train_set.Rating == 3]\ndf_4 = train_set[train_set.Rating == 4]\ndf_5 = train_set[train_set.Rating == 5]\n\n# Upsample minority classes\ntarget_n = 2000\ndf_2_ups = resample(df_2, replace=True, n_samples=target_n, random_state=42)\ndf_3_ups = resample(df_3, replace=True, n_samples=target_n, random_state=42)\ndf_4_ups = resample(df_4, replace=True, n_samples=target_n, random_state=42)\n\n# Combine and shuffle\ntrain_balanced = pd.concat([df_1, df_2_ups, df_3_ups, df_4_ups, df_5])\ntrain_balanced = train_balanced.sample(frac=1, random_state=42).reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T15:42:57.456575Z","iopub.execute_input":"2026-01-20T15:42:57.457247Z","iopub.status.idle":"2026-01-20T15:42:57.476265Z","shell.execute_reply.started":"2026-01-20T15:42:57.457216Z","shell.execute_reply":"2026-01-20T15:42:57.475358Z"}},"outputs":[],"execution_count":6},{"id":"354ff13f-6e6d-4e2d-9e49-6673201b9df7","cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\nfrom sklearn.metrics import f1_score, classification_report\nimport numpy as np\n\n# 1. Initialize Tokenizer\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\n# 2. Custom Dataset Class\nclass ReviewDataset(Dataset):\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        # Extract features for this index\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        if self.labels is not None:\n            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n        return item\n\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\n# 3. Tokenize Data (Using max_length 128 for efficiency)\ntrain_encodings = tokenizer(train_balanced['cleaned_text'].tolist(), truncation=True, padding=True, max_length=128)\nval_encodings = tokenizer(val_set['cleaned_text'].tolist(), truncation=True, padding=True, max_length=128)\n\n# 4. Prepare Datasets (Rating 1-5 mapped to 0-4)\ntrain_dataset = ReviewDataset(train_encodings, (train_balanced['Rating'] - 1).tolist())\nval_dataset = ReviewDataset(val_encodings, (val_set['Rating'] - 1).tolist())\n\n# 5. Define Metric Function for Weighted F1\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    # This is the exact metric required by your assignment\n    weighted_f1 = f1_score(labels, preds, average='weighted')\n    return {'weighted_f1': weighted_f1}\n\n# 6. Load Model\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=5)\n\n# 7. Training Arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    eval_strategy=\"epoch\",      # Changed from eval_strategy for compatibility\n    save_strategy=\"epoch\",\n    learning_rate=3e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"weighted_f1\", # Use F1 to pick the best model\n    logging_dir='./logs',\n    report_to=\"none\"\n)\n\n# 8. Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics\n)\n\n# 9. Train and Evaluate\ntrainer.train()\neval_results = trainer.evaluate()\nprint(f\"Final Validation Results: {eval_results}\")\n\n# 10. Generate Final Predictions for Test Set\ntest_encodings = tokenizer(test_df['cleaned_text'].tolist(), truncation=True, padding=True, max_length=128)\ntest_dataset = ReviewDataset(test_encodings)\n\nraw_preds = trainer.predict(test_dataset)\ntest_preds = np.argmax(raw_preds.predictions, axis=-1) + 1 # Convert 0-4 back to 1-5\n\n# Save submission\ntest_df['Star Rating'] = test_preds\ntest_df[['id', 'Star Rating']].to_csv('predictions.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T15:43:02.034330Z","iopub.execute_input":"2026-01-20T15:43:02.035078Z","iopub.status.idle":"2026-01-20T15:46:43.199296Z","shell.execute_reply.started":"2026-01-20T15:43:02.035045Z","shell.execute_reply":"2026-01-20T15:46:43.198747Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0035e4b68eb940a6bee9d295e7e9c400"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='918' max='918' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [918/918 03:26, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Weighted F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.845947</td>\n      <td>0.708309</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.758500</td>\n      <td>0.846720</td>\n      <td>0.734698</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.758500</td>\n      <td>0.893501</td>\n      <td>0.702958</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Final Validation Results: {'eval_loss': 0.8467204570770264, 'eval_weighted_f1': 0.7346981859889221, 'eval_runtime': 2.7527, 'eval_samples_per_second': 413.778, 'eval_steps_per_second': 13.078, 'epoch': 3.0}\n","output_type":"stream"}],"execution_count":7},{"id":"7a73d471-040c-4dab-80bd-7b49a1e571e1","cell_type":"code","source":"pd.prediction.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-20T15:49:33.509506Z","iopub.execute_input":"2026-01-20T15:49:33.510025Z","iopub.status.idle":"2026-01-20T15:49:33.515235Z","shell.execute_reply.started":"2026-01-20T15:49:33.509999Z","shell.execute_reply":"2026-01-20T15:49:33.514370Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1214321784.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'prediction'"],"ename":"AttributeError","evalue":"module 'pandas' has no attribute 'prediction'","output_type":"error"}],"execution_count":11},{"id":"e6c43e6c-2fa0-47bf-bebb-41a370d0aae1","cell_type":"code","source":"!pip install -U accelerate transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T18:18:23.607050Z","iopub.execute_input":"2026-01-19T18:18:23.607421Z","iopub.status.idle":"2026-01-19T18:18:27.043263Z","shell.execute_reply.started":"2026-01-19T18:18:23.607395Z","shell.execute_reply":"2026-01-19T18:18:27.042492Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\nRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (26.0rc2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\nRequirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.1rc0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n","output_type":"stream"}],"execution_count":18}]}